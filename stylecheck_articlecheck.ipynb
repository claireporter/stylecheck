{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8deceb805934437f8789d8ebd15c73d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de449735a8c54520b60678af5e47bbe8",
              "IPY_MODEL_40f7f682699b4fc58144d7782569e678",
              "IPY_MODEL_c53ed368da1c4dc19b3e80a7fda5b19e",
              "IPY_MODEL_3890312ee41743a29b1c080c84ea937d"
            ],
            "layout": "IPY_MODEL_09d8f835f1dd49389565a90342b82ff5"
          }
        },
        "de449735a8c54520b60678af5e47bbe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "gpt-4o-mini"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_f0a23014c0f243ba8c62d15c8855da21",
            "style": "IPY_MODEL_d09fba9f76a14f6d9607b0736d6e82a0"
          }
        },
        "40f7f682699b4fc58144d7782569e678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Article:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_1ce26a42b74549cd862675c7d926106d",
            "placeholder": "Type the input text here",
            "rows": null,
            "style": "IPY_MODEL_d3b68b6a57bf4fe481d6200b8b1f2956",
            "value": "He easily adapts to the latest technology.\nPeople run the 1,500m and take part in a 5,000m race. \nHe was an early adopter of the new technology.\nThe newspaper goes to press at 10:30 PM.\nThe car collided with a pedestrian.\nHis excellency, the archbishop was there"
          }
        },
        "c53ed368da1c4dc19b3e80a7fda5b19e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Generate Output",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_2e6a800f3b7b45d8a26eedbfc02fa550",
            "style": "IPY_MODEL_ab682f5902a74f3b9221ff9af1852e28",
            "tooltip": ""
          }
        },
        "3890312ee41743a29b1c080c84ea937d": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_3fef8fed6aed4a068c335a2110fa81d0",
            "msg_id": "",
            "outputs": []
          }
        },
        "09d8f835f1dd49389565a90342b82ff5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0a23014c0f243ba8c62d15c8855da21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "50px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "30%"
          }
        },
        "d09fba9f76a14f6d9607b0736d6e82a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ce26a42b74549cd862675c7d926106d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "375px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "80%"
          }
        },
        "d3b68b6a57bf4fe481d6200b8b1f2956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e6a800f3b7b45d8a26eedbfc02fa550": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab682f5902a74f3b9221ff9af1852e28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "3fef8fed6aed4a068c335a2110fa81d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claireporter/stylecheck/blob/main/stylecheck_articlecheck.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "!pip install openai==0.28\n",
        "# tiktoken only needed for cost evaluation, not actual runnind of the model on the article\n",
        "!pip install tiktoken\n",
        "!pip install -qU \\\n",
        "    pinecone-client==3.0.2 \\\n",
        "    openai==0.28.0\n",
        "\n",
        "import openai\n",
        "# time only needed for speed evaluation\n",
        "import time\n",
        "# tiktoken only needed for cost evaluation, not actual runnind of the model on the article\n",
        "import tiktoken\n",
        "import json\n",
        "import os\n",
        "# pandas only used for reading google sheet file\n",
        "import pandas as pd\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from google.colab import drive, userdata\n",
        "\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "openai.api_key = api_key\n",
        "\n",
        "## form\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qXNyxmRT6yB4",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5a3e235-b072-42d1-d504-e2f7e530b8d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->openai==0.28) (0.2.0)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.52.2\n",
            "    Uninstalling openai-1.52.2:\n",
            "      Successfully uninstalled openai-1.52.2\n",
            "Successfully installed openai-0.28.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with open(\"style_check/requirements.txt\", \"w\") as f:\n",
        "#    f.write(\"openai==0.28.0\\n\")\n",
        "#    f.write(\"pandas\\n\")\n",
        "#    f.write(\"pinecone-client==3.0.2\\n\")\n",
        "#    f.write(\"tiktoken\\n\")\n",
        "#    f.write(\"ipywidgets\\n\")\n",
        "#    f.write(\"ipython\\n\")\n"
      ],
      "metadata": {
        "id": "niIWW_kODuhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nYN8dOUC33Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gs5AqMSi6v-r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUikxl3sECHY"
      },
      "outputs": [],
      "source": [
        "# Calculate Pinecone cost\n",
        "def calculate_pinecone_cost(read_units, write_units, storage_cost_per_gb, vector_size_in_bytes, num_vectors):\n",
        "    # Calculate storage cost\n",
        "    storage_cost = (num_vectors * vector_size_in_bytes) / 1e9 * storage_cost_per_gb\n",
        "\n",
        "    # Costs for read and write units\n",
        "    read_cost_per_unit = 16/1_000_000  # Adjust based on Pinecone pricing for read units\n",
        "    write_cost_per_unit = 4/1_000_000  # Adjust based on Pinecone pricing for write units\n",
        "    total_cost = (read_units * read_cost_per_unit) + (write_units * write_cost_per_unit) + storage_cost\n",
        "\n",
        "    return total_cost\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML('''\n",
        "<style>\n",
        "     label {\n",
        "         display: block;\n",
        "         margin-bottom: 5px;\n",
        "         font-weight: bold;\n",
        "         width: 100%;\n",
        "     }\n",
        "     select, textarea {\n",
        "         width: 100%;\n",
        "         padding: 10px;\n",
        "         margin-bottom: 15px;\n",
        "         border: 1px solid #ccc;\n",
        "         border-radius: 4px;\n",
        "         font-size: 14px;\n",
        "         height: 100%;\n",
        "     }\n",
        "     button {\n",
        "         background-color: #4CAF50;\n",
        "         color: white;\n",
        "         padding: 10px 20px;\n",
        "         border: none;\n",
        "         border-radius: 4px;\n",
        "         cursor: pointer;\n",
        "         font-size: 16px;\n",
        "     }\n",
        "     button:hover {\n",
        "         background-color: #45a049;\n",
        "     }\n",
        " </style>\n",
        "'''))\n",
        "\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=['gpt-4o-mini'],\n",
        "    value='gpt-4o-mini',\n",
        "    description='Model:',\n",
        "    layout=widgets.Layout(width='30%', height='50px')\n",
        ")\n",
        "\n",
        "\n",
        "input_text = widgets.Textarea(value='He easily adapts to the latest technology.\\nPeople run the 1,500m and take part in a 5,000m race. \\nHe was an early adopter of the new technology.\\nThe newspaper goes to press at 10:30 PM.\\nThe car collided with a pedestrian.\\nHis excellency, the archbishop was there',\n",
        "    placeholder='Type the input text here',\n",
        "    description='Article:',\n",
        "    layout=widgets.Layout(width='80%', height='375px')\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "generate_button = widgets.Button(description=\"Generate Output\")\n",
        "\n",
        "\n",
        "def on_generate_button_click(b):\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        try:\n",
        "          generate_output(input_text.value, model_dropdown.value)\n",
        "        except AttributeError as e:\n",
        "          print(\"AttributeError:\", e)\n",
        "        except Exception as e:\n",
        "          print(\"An error occurred:\", e)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S_PF6JKzkvex",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "904e0af3-1d3a-4e86-f1ac-f89e069c9031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "     label {\n",
              "         display: block;\n",
              "         margin-bottom: 5px;\n",
              "         font-weight: bold;\n",
              "         width: 100%;\n",
              "     }\n",
              "     select, textarea {\n",
              "         width: 100%;\n",
              "         padding: 10px;\n",
              "         margin-bottom: 15px;\n",
              "         border: 1px solid #ccc;\n",
              "         border-radius: 4px;\n",
              "         font-size: 14px;\n",
              "         height: 100%;\n",
              "     }\n",
              "     button {\n",
              "         background-color: #4CAF50;\n",
              "         color: white;\n",
              "         padding: 10px 20px;\n",
              "         border: none;\n",
              "         border-radius: 4px;\n",
              "         cursor: pointer;\n",
              "         font-size: 16px;\n",
              "     }\n",
              "     button:hover {\n",
              "         background-color: #45a049;\n",
              "     }\n",
              " </style>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_token_usage_cost(tokens, cost_per_million):\n",
        "     cost_per_one_token = cost_per_million / 1000000\n",
        "     return (tokens * cost_per_one_token)\n",
        "\n",
        "\n",
        "# Load guidelines as google sheet and do pinecone prep ie create pinecone embeddings\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#pinecone_key = userdata.get('PINECONE_API_KEY')\n",
        "#pinecone = Pinecone(api_key=pinecone_key)\n",
        "\n",
        "with open('/content/drive/MyDrive/PCapi.txt', 'r') as file:\n",
        "    PCApi = file.read().strip()\n",
        "\n",
        "pinecone = Pinecone(api_key=PCApi)\n",
        "\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)\n",
        "\n",
        "# Pinecone index\n",
        "index_name = 'semantic-search-fast3'\n",
        "pinecone.delete_index(index_name)\n",
        "\n",
        "pinecone.create_index(\n",
        "    name='semantic-search-fast3',\n",
        "    #dimension=1536,  # 1536 for text-embedding-ada-002 or 3072 for text-embedding-3-small\n",
        "    dimension=3072,  # 3072 for text-embedding-3-large\n",
        "    metric='dotproduct',\n",
        "    spec=spec\n",
        ")\n",
        "index = pinecone.Index('semantic-search-fast3')\n",
        "\n",
        "# Load Excel file\n",
        "#xlsx_file_path = \"/content/StyleGuideforMyOrganizationSimplifiedRAGAndPromptTestingWithHeaders20recordsHoned_type3.xlsx\"\n",
        "#xlsx_file_path = \"/content/StyleGuideforMyOrganizationSimplifiedRAGAndPromptTestingWithHeaders20recordsHoned_type3_700_faked_rules.xlsx\"\n",
        "#xlsx_file_path = \"/content/Style_Guide_2024_CP_copy_24_10_18_faked.xlsx\"\n",
        "xlsx_file_path = \"/content/style_check/data/Style_Guide_2024_CP_copy_24_10_18_faked.xlsx\"\n",
        "df = pd.read_excel(xlsx_file_path, sheet_name=\"Data Sheet\")\n",
        "\n",
        "df['Style Point'] = df['Style Point'].str.replace('**', '', regex=False)\n",
        "\n",
        "# Replace null values in keywords with 'Our Style' column\n",
        "df['Our Style'].fillna(df['Keywords'], inplace=True)\n",
        "df['Our Style'].replace('', 'placeholder text', inplace=True)\n",
        "df['Our Style'].fillna('placeholder text', inplace=True)\n",
        "\n",
        "df_filtered = df[df[\"Type\"] == 3]\n",
        "\n",
        "# Prepare lists of rules and entries\n",
        "ID_list = df_filtered[\"ID\"].tolist()\n",
        "type_list = df_filtered[\"Type\"].tolist()\n",
        "keywords_list = df_filtered[\"Keywords\"].tolist()\n",
        "#rule_list = (df_filtered[\"Style Point\"] + ' ' + df_filtered[\"Our Style\"]).tolist()\n",
        "#rule_list = (df_filtered[\"Style Point\"] + ' ' + df_filtered[\"Our Style\"] + ' Examples of correct usage: ' + df_filtered[\"Correct usage\"] + ' Examples of incorrect usage: ' + df_filtered[\"Incorrect usage\"]).tolist()\n",
        "\n",
        "rule_list = df_filtered.apply(\n",
        "    lambda row: (\n",
        "        row[\"Style Point\"] + ' ' +\n",
        "        row[\"Our Style\"] +\n",
        "        (' Examples of correct usage: ' + row[\"Correct usage\"] if pd.notna(row[\"Correct usage\"]) and row[\"Correct usage\"] != '' else '') +\n",
        "        (' Examples of incorrect usage: ' + row[\"Incorrect usage\"] if pd.notna(row[\"Incorrect usage\"]) and row[\"Incorrect usage\"] != '' else '')\n",
        "    ),\n",
        "    axis=1\n",
        ").tolist()\n",
        "\n",
        "#print(f\"rule_list: {rule_list}\")\n",
        "# TO REMOVE - Keep track of rule ids to check what is going into embeddings.\n",
        "rule_ids = df_filtered[\"ID\"].tolist()\n",
        "\n",
        "def narrow_down_rules(textToValidate):\n",
        "\n",
        "  top_k = 10  # Limit number of results to return by pinecone query to ensure efficiency.\n",
        "\n",
        "  matched_rule_ids_all = []\n",
        "  matched_rule_ids_string_search_with_ai = []\n",
        "  matched_rule_ids_dense = []\n",
        "  matched_rule_ids_always = []\n",
        "\n",
        "  # Type 2 rules - Do string search for rules with a list of keywords to match in the article\n",
        "\n",
        "  #print(f\"Type 2 search: text to validate (to delete print): {textToValidate}\")\n",
        "\n",
        "\n",
        "  for i in range(len(df)):\n",
        "      myType = df.loc[i, 'Type']\n",
        "      rule_id = df.loc[i, 'ID']\n",
        "      keyword_csv = df.loc[i, 'Keywords']\n",
        "      keyword_csv = clean_comma_delimited_list(keyword_csv)\n",
        "\n",
        "      if myType == 2:\n",
        "          #print(f\"textToValidate: '{textToValidate}'\")\n",
        "          #print(f\"Rule ID of a type 2 rule: '{rule_id}'\")\n",
        "          relevant_sentences = []\n",
        "          matched = False  # Reset matched for each rule\n",
        "\n",
        "          for keyword in keyword_csv.split(','):\n",
        "              keyword = keyword.strip().lower()\n",
        "              #print(f\"Type 2 search: Checking keyword  (to delete print) : {keyword}\")\n",
        "\n",
        "              # find matches to keywords of complete words only\n",
        "              matched_sentences = [\n",
        "                sentence.strip() for sentence in textToValidate.split('.')\n",
        "                if (f\" {keyword} \" in f\" {sentence.lower()} \" or  # keyword in the middle\n",
        "                  sentence.lower().startswith(f\"{keyword} \") or  # keyword at the start\n",
        "                  sentence.lower().endswith(f\" {keyword}\"))      # keyword at the end\n",
        "              ]\n",
        "\n",
        "              if matched_sentences:\n",
        "                  matched = True\n",
        "                  myID = int(df.loc[i, 'ID'])\n",
        "                  myKeywords = df.loc[i, 'Keywords']\n",
        "                  relevant_sentences.extend(matched_sentences)\n",
        "                  break  # Stop checking further if a match is found within this rule\n",
        "          if matched:\n",
        "            sentenceCount = 0\n",
        "            for sentence in relevant_sentences:\n",
        "                sentenceCount += 1\n",
        "                print(f\"Found {myKeywords} in {sentenceCount} sentences, Rule ID: {myID}\")\n",
        "                print(f\"Sentence: '{sentence}'\")\n",
        "                if myID not in matched_rule_ids_string_search_with_ai:\n",
        "                  matched_rule_ids_string_search_with_ai.append(str(int(myID)))\n",
        "\n",
        "          # Convert ID array to strings - to match format of dense array\n",
        "          matched_rule_ids_string_search_with_ai = [str(item) for item in matched_rule_ids_string_search_with_ai]\n",
        "\n",
        "      elif myType == 4:\n",
        "        myID = int(df.loc[i, 'ID'])\n",
        "        matched_rule_ids_always.append(str(int(myID)))\n",
        "\n",
        "  # Type 3 rules - do Pinecone meaning search using dense vector,\n",
        "  # for rules where search by keywords wouldn't work\n",
        "\n",
        "  # Generate embeddings a second time, for the article text\n",
        "  # (in order to query against the stored rule embeddings)\n",
        "  query_dense = get_embeddings([textToValidate])[0]\n",
        "\n",
        "  # ========================================================\n",
        "\n",
        "  # Calculate cost of embeddings creation for rule_list ...\n",
        "  encoder = tiktoken.encoding_for_model(\"text-embedding-3-large\")\n",
        "  embedding_tokens_article = len(encoder.encode(textToValidate))\n",
        "\n",
        "  # - for Open AI embeddings model\n",
        "\n",
        "  # Adjust according to open ai model pricing\n",
        "  text_embedding_3_large_tokens_cost_per_million = 0.13\n",
        "# gothere1\n",
        "  embedding_article_cost = calculate_token_usage_cost(embedding_tokens_article, text_embedding_3_large_tokens_cost_per_million)\n",
        "\n",
        "  print(f\"Embedding tokens used for article: {embedding_tokens_article}\")\n",
        "  print(f\"Cost for embeddings for article: ${embedding_article_cost:.6f}\")\n",
        "\n",
        "  # = for Pinecone\n",
        "\n",
        "  query_vectors = 1  # Number of query vectors (textToValidate)\n",
        "  read_units = query_vectors * (top_k / 10)  # 1 read unit per 10 results\n",
        "\n",
        "  pinecone_cost_query = calculate_pinecone_cost(read_units, 0, 0, 0, 0)  # Only read units\n",
        "  print(f\"Pinecone cost currently free on starter plan but estimate based on lowest rate for standard plan:\\n  Pinecone Cost for querying: ${pinecone_cost_query:.6f}\")\n",
        "\n",
        "\n",
        "  #1 Query the index with dense embedding\n",
        "  query_response = index.query(\n",
        "        top_k=top_k,\n",
        "        vector=query_dense,  # Running query on dense vector\n",
        "        include_metadata=True\n",
        "  )\n",
        "\n",
        "  for match in query_response['matches']:\n",
        "          keywords = match[\"metadata\"][\"keywords\"]\n",
        "          score = match[\"score\"]\n",
        "          myID = str(int(match[\"metadata\"][\"ID\"]))\n",
        "          myType = match[\"metadata\"][\"type\"]\n",
        "\n",
        "\n",
        "          print(\"myID\", myID, \" score:\", score, \"keywords:\", keywords)\n",
        "          # If too many pinecone rules found, up the threshold. Improve the rule descriptions.\n",
        "          #if score > 0.17:  # Apply 0.2 threshold for dense vector matches\n",
        "          if score > 0.28:  # Apply 0.2 threshold for dense vector matches\n",
        "            matched_rule_ids_dense.append(str(int(myID)))\n",
        "\n",
        "\n",
        "  matched_rules_filtered = [item for item in matched_rule_ids_string_search_with_ai if item not in matched_rule_ids_dense]\n",
        "\n",
        "  matched_rule_ids_all = matched_rule_ids_string_search_with_ai + matched_rule_ids_dense + matched_rule_ids_always\n",
        "\n",
        "  print(\"Matched Rule IDs Type 2 - String search on keywords:\", matched_rule_ids_string_search_with_ai)\n",
        "\n",
        "  print(f\"Matched Rule IDs Type 3 - Meaning search on keywords - Pinecone: \", matched_rule_ids_dense)\n",
        "\n",
        "  print(\"Matched Rule IDs Type 4 - Always checked via prompt:\", matched_rule_ids_always)\n",
        "\n",
        "  print(\"All Matched Rule IDs:\", matched_rule_ids_all)\n",
        "\n",
        "  # Generate a formatted string of rules for matched_rule_ids_all\n",
        "  string_of_rules = \"\"\n",
        "  for rule_id in matched_rule_ids_all:\n",
        "        row = df.loc[df['ID'] == int(rule_id)]\n",
        "        if not row.empty:\n",
        "            rule_text = (\n",
        "                f\"Rule {row['ID'].values[0]}\\n\"\n",
        "                f\"{row['Style Point'].values[0]}\\n\"\n",
        "                f\"{row['Our Style'].values[0]}\\n\"\n",
        "            )\n",
        "            # Add correct and incorrect usage examples if available\n",
        "            if pd.notna(row['Correct usage'].values[0]):\n",
        "                rule_text += f\"This is correct usage:\\n{row['Correct usage'].values[0]}\\n\"\n",
        "            if pd.notna(row['Incorrect usage'].values[0]):\n",
        "                rule_text += f\"This is incorrect usage:\\n{row['Incorrect usage'].values[0]}\\n\"\n",
        "\n",
        "            # Append this rule text to the overall string with line breaks\n",
        "            string_of_rules += f\"{rule_text}\\n\\n\"\n",
        "\n",
        "  print(f\"=============\")\n",
        "  return matched_rule_ids_all, embedding_tokens_article, embedding_article_cost, string_of_rules;\n",
        "\n",
        "\n",
        "# Generate embeddings using OpenAI's embeddings models\n",
        "def get_embeddings(rule_list):\n",
        "    response = openai.Embedding.create(\n",
        "        input=rule_list,\n",
        "        model=\"text-embedding-3-large\" # had best results so far\n",
        "        #model=\"text-embedding-ada-002\"\n",
        "        #model=\"text-embedding-3-small\"\n",
        "    )\n",
        "    embeddings = [embedding['embedding'] for embedding in response['data']]\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "# Generate embeddings the first time, from the rules, to store in pinecone\n",
        "embeddings = get_embeddings(rule_list)\n",
        "\n",
        "# Calculate costs for embeddings creation for rule_list\n",
        "\n",
        "# - for Open AI model\n",
        "encoder = tiktoken.encoding_for_model(\"text-embedding-3-large\")\n",
        "embedding_tokens_rule_list = sum([len(encoder.encode(text)) for text in rule_list])\n",
        "text_embedding_3_large_tokens_cost_per_million = 0.13\n",
        "embedding_rule_list_token_cost = calculate_token_usage_cost(embedding_tokens_rule_list, text_embedding_3_large_tokens_cost_per_million)\n",
        "\n",
        "# Print token and cost information\n",
        "print(f\"Embedding tokens used for rule_list: {embedding_tokens_rule_list}\")\n",
        "print(f\"Cost for embeddings for rule_list: ${embedding_rule_list_token_cost:.6f}\")\n",
        "\n",
        "# - for Pinecone\n",
        "vector_size_in_bytes = 3072 * 4  # 3072 dimensions, float32 (4 bytes)\n",
        "num_upsert_vectors = len(embeddings)  # Number of vectors stored\n",
        "storage_cost_per_gb = 0.33  # Adjust based on your pricing region\n",
        "write_units = num_upsert_vectors / 1000  # 1 write unit per 1000 vectors\n",
        "\n",
        "# Calculate Pinecone cost for storing and upserting vectors\n",
        "pinecone_cost_rules = calculate_pinecone_cost(0, write_units, storage_cost_per_gb, vector_size_in_bytes, num_upsert_vectors)\n",
        "print(f\"Pinecone Cost currently free on starter plan but estimate based on lowest rate for standard plan - for storing and upserting rules: ${pinecone_cost_rules:.6f}\")\n",
        "\n",
        "# Prepare records for upsert\n",
        "records = []\n",
        "for i in range(len(embeddings)):\n",
        "    ind_dic = {\n",
        "        'id': f'vec{i+1}',\n",
        "        'values': embeddings[i],\n",
        "        'metadata': {\n",
        "            'keywords': keywords_list[i],\n",
        "            'rule': rule_list[i],\n",
        "            #'ID': str(df.loc[i, 'ID']),\n",
        "            'ID': ID_list[i],\n",
        "            #'type': str(df.loc[i, 'Type']),\n",
        "            'type': type_list[i],\n",
        "        }\n",
        "    }\n",
        "    records.append(ind_dic)\n",
        "\n",
        "# Remove final , in csv of keywords if exists\n",
        "def clean_comma_delimited_list(myListString):\n",
        "  if myListString.endswith(','):\n",
        "      myListString = myListString[:-1]\n",
        "  return myListString\n",
        "\n",
        "\n",
        "index.upsert(vectors=records)\n"
      ],
      "metadata": {
        "id": "Svzgab58nWAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ad5b686-b5fd-47ab-d6bd-26aa070202df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-034dfcb9bea6>:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Our Style'].fillna(df['Keywords'], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding tokens used for rule_list: 11288\n",
            "Cost for embeddings for rule_list: $0.001467\n",
            "Pinecone Cost currently free on starter plan but estimate based on lowest rate for standard plan - for storing and upserting rules: $0.000317\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'upserted_count': 78}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YPDsEkrMhArW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GENERATE PROMPT FROM rule_ids\n",
        "\n",
        "# Use function call to set the json output format\n",
        "\n",
        "\n",
        "result_functions = [\n",
        "    {\n",
        "        'name': 'validate_article_compliance',\n",
        "        'description': 'Validate article compliance with rules',\n",
        "        'parameters': {\n",
        "            'type': 'object',\n",
        "            'properties': {\n",
        "                'isArticleCompliantWithAllRules': {\n",
        "                    'type': 'boolean',\n",
        "                    'description': 'Indicates whether the article complies with all rules.'\n",
        "                },\n",
        "                'issuesOfNonCompliance': {\n",
        "                    'type': 'array',\n",
        "                    'items': {\n",
        "                        'type': 'object',\n",
        "                        'properties': {\n",
        "                            'ruleID': {\n",
        "                                'type': 'string',\n",
        "                                'description': 'Rule ID'\n",
        "                            },\n",
        "                            'stylePoint': {\n",
        "                                'type': 'string',\n",
        "                                'description': 'Style Point'\n",
        "                            },\n",
        "                            'ourStyle': {\n",
        "                                'type': 'string',\n",
        "                                'description': 'Our Style'\n",
        "                            },\n",
        "                            'textWithProblem': {\n",
        "                                'type': 'string',\n",
        "                                'description': 'Text containing the compliance issue.'\n",
        "                            },\n",
        "                            'suggestedAlternative': {\n",
        "                                'type': 'string',\n",
        "                                'description': 'Suggested alternative text.'\n",
        "                            },\n",
        "                            'explanation': {\n",
        "                                'type': 'string',\n",
        "                                'description': 'Explanation of the issue.'\n",
        "                            }\n",
        "                        }\n",
        "                    },\n",
        "                    'description': 'List of compliance issues.'\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "def get_token_costs(model):\n",
        "\n",
        "    # Define costs for gpt-4o and gpt-4o-mini\n",
        "    # Prices as per 2024\n",
        "    gpt4o_input_tokens_cost_per_million = 2.5 # ($s)\n",
        "    gpt4o_output_tokens_cost_per_million = 10 # ($s)\n",
        "\n",
        "    gpt4o_mini_input_tokens_cost_per_million =  0.150 # ($s)\n",
        "    gpt4o_mini_output_tokens_cost_per_million = 0.6 # ($s)\n",
        "\n",
        "    # Select case structure using if-elif\n",
        "    if model == \"gpt-4o\":\n",
        "        input_tokens_cost_per_million = gpt4o_input_tokens_cost_per_million\n",
        "        output_tokens_cost_per_million = gpt4o_output_tokens_cost_per_million\n",
        "    elif model == \"gpt-4o-mini\":\n",
        "        input_tokens_cost_per_million = gpt4o_mini_input_tokens_cost_per_million\n",
        "        output_tokens_cost_per_million = gpt4o_mini_output_tokens_cost_per_million\n",
        "    else:\n",
        "        raise ValueError(f\"Model {model} is not supported.\")\n",
        "\n",
        "    # Return the costs for input and output tokens\n",
        "    return input_tokens_cost_per_million, output_tokens_cost_per_million\n",
        "\n",
        "def limit_rule_ids(matched_rule_ids):\n",
        "\n",
        "    limit_rules_genai = 20\n",
        "    #20 rules for mini - goes through without an error, whereas even 10 rules can fail for gpt-4o\n",
        "    # Limit to less than x rules\n",
        "    if len(matched_rule_ids) >= limit_rules_genai:\n",
        "\n",
        "        matched_rule_ids = matched_rule_ids[:limit_rules_genai]\n",
        "\n",
        "    # Convert the list of numbers to a comma-delimited string\n",
        "    rule_ids_comma_delimited_string = \",\".join(map(str, matched_rule_ids))\n",
        "\n",
        "    return rule_ids_comma_delimited_string\n",
        "\n",
        "\n",
        "def get_compliance_check(guideline, text, matched_rule_ids, model, embedding_tokens_article, embedding_article_cost, string_of_rules):\n",
        "\n",
        "    # limit no of rules to 20 or under, to prevent error 'Request too large for gpt-4o'\n",
        "    rule_ids_formatted_for_prompt = limit_rule_ids(matched_rule_ids)\n",
        "\n",
        "    print('matched_rule_ids', matched_rule_ids)\n",
        "\n",
        "\n",
        "    # Generate prompt preamble before listing all rules\n",
        "    prompt = f\"\"\"Here are the rules. \\n\\n\n",
        "        Go through rule IDs {string_of_rules} only) provided in the uploaded style guide (ignore all other rules), and in each case, state if the provided input text is compliant with each of the rules.\n",
        "        In each case, state if the provided input text is compliant with each of the following rules.\n",
        "        If the context is not found, do not apply the guideline, and state that the text is compliant.\n",
        "        If the context is not certain, ignore the guideline and state that the rule is compliant.\n",
        "        Provide an explanation in each case.\n",
        "        Return a json array as a result.\n",
        "        The json must have the format as stated in the function call\n",
        "        There should be one result per rule ID that is not compliant in one array.\n",
        "        Only the provided rules should be used and that any external information or assumptions should be excluded.\n",
        "        Then there should be one overall value of isArticleCompliantWithAllRules (true/false). When a ruleID is output in Json format it as an Integer\"\"\"\n",
        "\n",
        "    # Alternative prompt - also works\n",
        "    #prompt = f\"\"\"You are the editor-in-chief at an international news agency, tasked with revising and verifying all copy before publication.\n",
        "    #  Your responsibilities include:\n",
        "    #  Reading Text Inputs: Analyze each text input against established guidelines.\n",
        "    #  Go through rules  {string_of_rules}\n",
        "    #  Output Requirements:\n",
        "    #  Provide explanations for each non-compliant rule.\n",
        "    #  Return a JSON array with:\n",
        "    #  Each non-compliant rule ID.\n",
        "    #  A single overall value of isArticleCompliantWithAllRules (true/false).\"\"\"\n",
        "\n",
        "\n",
        "    # print(\"Generated Prompt:\\n\", prompt)\n",
        "\n",
        "    send_prompt = (\n",
        "        f\"{prompt}\\n\\n\"\n",
        "        f\"Guideline: {guideline}\\n\\n\"\n",
        "        f\"Text: {text}\\n\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": send_prompt}],\n",
        "        temperature=0,\n",
        "        functions = result_functions,\n",
        "        function_call = 'auto'\n",
        "    )\n",
        "\n",
        "    # Get costs of tokens depending on model used\n",
        "    genai_input_tokens_cost_per_million, genai_output_tokens_cost_per_million = get_token_costs(model)\n",
        "\n",
        "    # Get the input tokens\n",
        "    usage = response['usage']\n",
        "    genAI_input_tokens = usage['prompt_tokens']\n",
        "    genAI_input_token_cost = calculate_token_usage_cost(genAI_input_tokens, genai_input_tokens_cost_per_million)\n",
        "    print(f\"GenAI Input tokens: {genAI_input_tokens} - cost this time: ${genAI_input_token_cost:.6f}\")\n",
        "\n",
        "    # Get the output tokens\n",
        "    genAI_output_tokens = usage['completion_tokens']\n",
        "    genAI_output_token_cost = calculate_token_usage_cost(genAI_output_tokens, genai_output_tokens_cost_per_million)\n",
        "    print(f\"GenAI Output tokens: {genAI_output_tokens} - cost this time: ${genAI_output_token_cost:.6f}\")\n",
        "\n",
        "    genAI_token_cost = genAI_input_token_cost + genAI_output_token_cost\n",
        "    print(f\"Total genAI cost this time: ${genAI_token_cost:.6f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    encoder = tiktoken.encoding_for_model(model)\n",
        "    tokens = encoder.encode(send_prompt)\n",
        "\n",
        "    # cost true as of 2024\n",
        "    text_embedding_3_large_tokens_cost_per_million = 0.13\n",
        "\n",
        "    tokens_embedding = sum([len(encoder.encode(send_prompt)) for text in rule_list])\n",
        "    # gothere2\n",
        "    embedding_token_cost = calculate_token_usage_cost(tokens_embedding, text_embedding_3_large_tokens_cost_per_million)\n",
        "    print(f\"Embedding tokens used for rules: {tokens_embedding} - Cost this time: ${embedding_token_cost:.6f}\")\n",
        "\n",
        "    total_openAI_cost = embedding_token_cost + genAI_token_cost\n",
        "    print(f\"Total OpenAI cost: ${total_openAI_cost:.6f}\")\n",
        "    print(f\"Gen AI call time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "    json_response = '';\n",
        "\n",
        "    # Attempt to parse function call arguments if they exist\n",
        "    try:\n",
        "        json_response = json.loads(response.choices[0].message.get(\"function_call\", {}).get(\"arguments\", \"{}\"))\n",
        "        formatted_json = json.dumps(json_response, indent=2)\n",
        "    except json.JSONDecodeError:\n",
        "        formatted_json = \"Invalid JSON response or no function call arguments.\"\n",
        "\n",
        "    print('formatted_json', formatted_json)\n",
        "\n",
        "\n",
        "    return json_response, embedding_tokens_article, embedding_article_cost\n",
        "\n",
        "# Format of rule generated from sheet columns\n",
        "# {Rule ID} {Style Point} {Our Style}\n",
        "# correct Usage: {List of correct usage examples}\n",
        "# incorrect Usage: {List of incorrect usage examples}\n",
        "\n",
        "def generate_guideline_from_df(df):\n",
        "    guidelines = []\n",
        "    for i, row in df.iterrows():\n",
        "        parts = []  # List to hold the parts of the guideline\n",
        "\n",
        "        if pd.notna(row['ID']):\n",
        "            parts.append(f\"Rule {row['ID']}\")\n",
        "        if pd.notna(row['Style Point']):\n",
        "            parts.append(f\"{row['Style Point']}\")\n",
        "        if pd.notna(row['Our Style']):\n",
        "            parts.append(f\"{row['Our Style']}\")\n",
        "        if pd.notna(row['Correct usage']):\n",
        "            parts.append(f\"This type of usage is correct: {row['Correct usage']}\")\n",
        "        if pd.notna(row['Incorrect usage']):\n",
        "            parts.append(f\"This type of usage is incorrect: {row['Incorrect usage']}\")\n",
        "\n",
        "        # Join the parts with a newline, only if there are any parts to join\n",
        "        if parts:\n",
        "            guideline = \"\\n\".join(parts)\n",
        "            guidelines.append(guideline)\n",
        "\n",
        "    return \"\\n\\n\".join(guidelines)  # Separate each guideline by an extra newline\n",
        "\n",
        "\n",
        "# run prompt on open ai's gpt-4o model as this had the best results so far, testing with mini to see if results are as good, sa it's cheaper\n",
        "\n",
        "def generate_output(example, model):\n",
        "\n",
        "    matched_rule_ids, embedding_tokens_article, embedding_article_cost, string_of_rules = narrow_down_rules(example)\n",
        "    guideline = generate_guideline_from_df(df)\n",
        "    response = get_compliance_check(guideline, example, matched_rule_ids, model, embedding_tokens_article, embedding_article_cost, string_of_rules)\n",
        "\n",
        "generate_button.on_click(on_generate_button_click)\n",
        "\n",
        "display(widgets.VBox([model_dropdown, input_text, generate_button, output]))\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata"
      ],
      "metadata": {
        "id": "FVZnxqizuSIP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482,
          "referenced_widgets": [
            "8deceb805934437f8789d8ebd15c73d0",
            "de449735a8c54520b60678af5e47bbe8",
            "40f7f682699b4fc58144d7782569e678",
            "c53ed368da1c4dc19b3e80a7fda5b19e",
            "3890312ee41743a29b1c080c84ea937d",
            "09d8f835f1dd49389565a90342b82ff5",
            "f0a23014c0f243ba8c62d15c8855da21",
            "d09fba9f76a14f6d9607b0736d6e82a0",
            "1ce26a42b74549cd862675c7d926106d",
            "d3b68b6a57bf4fe481d6200b8b1f2956",
            "2e6a800f3b7b45d8a26eedbfc02fa550",
            "ab682f5902a74f3b9221ff9af1852e28",
            "3fef8fed6aed4a068c335a2110fa81d0"
          ]
        },
        "outputId": "1dba9ea9-96c5-46a8-af96-7cef716d4f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Dropdown(description='Model:', layout=Layout(height='50px', width='30%'), options=('gpt-4o-mini…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8deceb805934437f8789d8ebd15c73d0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip freeze > style_check/requirements.txt"
      ],
      "metadata": {
        "id": "QwuQxzkmAz_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/style_check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoB2YTBT9wmO",
        "outputId": "5f45b922-36d5-4ad8-ff13-76885a8df680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/style_check\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git init --initial-branch=main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LQBqj-G9257",
        "outputId": "59be07eb-64eb-4682-986e-ef80448c72b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "warning: re-init: ignored --initial-branch=main\n",
            "Reinitialized existing Git repository in /content/style_check/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global init.defaultBranch main"
      ],
      "metadata": {
        "id": "ir0XjRt2-JDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!git config --global user.email \"5698694+claireporter@users.noreply.github.com\"\n",
        "#!git config --global user.name \"claireporter\"\n",
        "\n",
        "!git remote set-url origin https://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@github.com/claireporter/stylecheck.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T8BLWP6Agwi",
        "outputId": "6e169d55-78f5-4eb4-a21f-6a87f1dd221d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: No such remote 'origin'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Initial commit of style_check project\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbuagaZ5-P9W",
        "outputId": "5d5ccb0d-3167-491f-806b-55dfe11f5ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author identity unknown\n",
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@b428b7e003aa.(none)')\n"
          ]
        }
      ]
    }
  ]
}